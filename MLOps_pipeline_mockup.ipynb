{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main resources:\n",
        "- https://brighteshun.medium.com/sentiment-analysis-part-1-finetuning-and-hosting-a-text-classification-model-on-huggingface-9d6da6fd856b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTaCz1i7WojQ"
      },
      "source": [
        "# 1. Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e_puNMeUsx2p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
            "[notice] To update, run: C:\\Users\\jared\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# !pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "#finetuning\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline, AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer, DistilBertTokenizerFast, DefaultDataCollator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXM_PsOvWrrs"
      },
      "source": [
        "# 2. Run sentiment analysis predictions by using Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGGZsv-5sBxB"
      },
      "source": [
        "Data: https://www.airlinequality.com/airline-reviews/scoot/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "iR3mTppxsGj7"
      },
      "outputs": [],
      "source": [
        "sample_data = 'I travelled with my sister, my elderly parent and my toddler son taking Scoot from Haikou to Singapore. The journey was a pleasant one despite the episode of my child became very unwell, alternating between drowsy and cranky in the mid of the journey. We sought the crew for help. They are very professional, helpful and friendly. They checked us out first, discussed with their captain and then prep us on the travel duration and preparation on ground. Besides, they offered ice pack/wet kitchen towels along the way. Nearing to the destination, they even moved me and my son to the front row. The flight arrived 20 minutes earlier. After that, they connected me and my son with the ground crew Firliza who accompanied us to airport clinic for medical treatment.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVAp8Urhs3Z7",
        "outputId": "710fe144-e463-4e73-c7b6-e345a30308be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"finetuning_task\": \"sst-2\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"NEGATIVE\",\n",
            "    \"1\": \"POSITIVE\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"NEGATIVE\": 0,\n",
            "    \"POSITIVE\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"finetuning_task\": \"sst-2\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"NEGATIVE\",\n",
            "    \"1\": \"POSITIVE\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"NEGATIVE\": 0,\n",
            "    \"POSITIVE\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
            "loading configuration file config.json from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"finetuning_task\": \"sst-2\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"NEGATIVE\",\n",
            "    \"1\": \"POSITIVE\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"NEGATIVE\": 0,\n",
            "    \"POSITIVE\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\tokenizer_config.json\n",
            "loading configuration file config.json from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"finetuning_task\": \"sst-2\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"NEGATIVE\",\n",
            "    \"1\": \"POSITIVE\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"NEGATIVE\": 0,\n",
            "    \"POSITIVE\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"finetuning_task\": \"sst-2\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"NEGATIVE\",\n",
            "    \"1\": \"POSITIVE\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"NEGATIVE\": 0,\n",
            "    \"POSITIVE\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWAcxn3HsLBh",
        "outputId": "55a41c51-9c22-4333-ac46-55884750ad7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9988247752189636}]"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = [sample_data]\n",
        "analysis = sentiment_pipeline(data)\n",
        "\n",
        "analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Fine tuning on custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_non_utf8(input_file, output_file, replace_char=''):\n",
        "  with open(input_file, 'r') as f:\n",
        "    content = f.read()\n",
        "  cleaned_content = ''.join(char if char.isascii() else replace_char for char in content)\n",
        "  with open(output_file, 'w') as f:\n",
        "    f.write(cleaned_content)\n",
        "\n",
        "\n",
        "def text_to_csv(input_file, output_file, delimiter=','):\n",
        "  \"\"\"\n",
        "  Converts a text file to a CSV file, using the specified delimiter.\n",
        "\n",
        "  Args:\n",
        "      input_file (str): Path to the input text file.\n",
        "      output_file (str): Path to the output CSV file.\n",
        "      delimiter (str, optional): Delimiter separating data in the text file. Defaults to ','.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(input_file, 'r') as input_file_handle, open(output_file, 'w', newline='') as output_csv:\n",
        "    reader = csv.reader(input_file_handle, delimiter=delimiter)\n",
        "    writer = csv.writer(output_csv)\n",
        "    # Assuming the first line contains headers (optional)\n",
        "    headers = next(reader)  # Read and store the header row (if present)\n",
        "    writer.writerow(headers)  # Write the header row to the CSV\n",
        "\n",
        "    for row in reader:\n",
        "      writer.writerow(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "data_path = 'demo.csv'\n",
        "cleaned_txt_path = 'cleaned.txt'\n",
        "cleaned_csv_path = 'cleaned.csv'\n",
        "\n",
        "remove_non_utf8(data_path, cleaned_txt_path) \n",
        "text_to_csv(cleaned_txt_path, cleaned_csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Was assigned last two row at seat 39F with the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Very rude male flight attendant. Accessed the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Upon check-in to Phuket airport, I could see t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>We had a thoroughly unpleasant experience chec...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I have to say that I didn't go into the flight...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  Was assigned last two row at seat 39F with the...      1\n",
              "1  Very rude male flight attendant. Accessed the ...      0\n",
              "2  Upon check-in to Phuket airport, I could see t...      0\n",
              "3  We had a thoroughly unpleasant experience chec...      0\n",
              "4  I have to say that I didn't go into the flight...      1"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(cleaned_csv_path)\n",
        "\n",
        "# Rename 'old_name' to 'new_name'\n",
        "df = df.rename(columns={'sentiment_score': 'label'})\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the train data => {train, eval}\n",
        "train, eval = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save splitted subsets\n",
        "\n",
        "train.to_csv(\"train_subset.csv\", index=False)\n",
        "eval.to_csv(\"eval_subset.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset('csv',\n",
        "                        data_files =\n",
        "                        {'train': 'train_subset.csv',\n",
        "                        'eval': 'eval_subset.csv'\n",
        "                        }\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.txt from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411\\vocab.txt\n",
            "loading file tokenizer.json from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411\\tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411\\tokenizer_config.json\n",
            "loading configuration file config.json from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411\\config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Create a tokenizer instance\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "debb0e21ab1e420ab433bf2a29d7ed7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e63f2fa7120f4b7cbe3ad95e3fa4d08e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8b2788adef24c90bf0c1be4c29552b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e77942aaf421448a89cfe468ddedf138",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#create a function to convert label\n",
        "def transform_labels(label):\n",
        "    return {'labels': label['label']}\n",
        "\n",
        "# let's tokenize the data for the model to be able to understand\n",
        "def tokenize_data(example):\n",
        "    return tokenizer(example['text'], padding='max_length')\n",
        "\n",
        "# Transform labels and remove the useless columns\n",
        "remove_label = ['label']\n",
        "remove_text = ['text']\n",
        "\n",
        "dataset = dataset.map(transform_labels, remove_columns=remove_label)\n",
        "dataset = dataset.map(tokenize_data, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 80\n",
              "    })\n",
              "    eval: Dataset({\n",
              "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 20\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 202,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"finetuning_task\": \"sst-2\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"NEGATIVE\",\n",
            "    \"1\": \"POSITIVE\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"NEGATIVE\": 0,\n",
            "    \"POSITIVE\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at C:\\Users\\jared/.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Loading a pretrain model while specifying the number of labels in our dataset for fine-tuning\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "# freeze all layers except the final classifier\n",
        "for param in model.distilbert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.pre_classifier.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "output_dir = \"finetune_sentiments_analysis_distilbert\"\n",
        "#set the training arguments\n",
        "training_args = TrainingArguments(\n",
        "                            num_train_epochs=1,\n",
        "                            evaluation_strategy='epoch',\n",
        "                            save_strategy='epoch',\n",
        "                            learning_rate=2e-5,\n",
        "                            load_best_model_at_end=True,\n",
        "                            output_dir=output_dir\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [],
      "source": [
        "# shuffle the datasets\n",
        "\n",
        "train_dataset = dataset['train'].shuffle(seed=10) \n",
        "eval_dataset = dataset['eval'].shuffle(seed=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "C:\\Users\\jared\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 80\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 10\n",
            "  Number of trainable parameters = 1538\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "317b3535bbbe439a982175e60ec10f75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 20\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fb3e74943b24753ac2f7d236997dd43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to finetune_sentiments_analysis_distilbert\\checkpoint-10\n",
            "Configuration saved in finetune_sentiments_analysis_distilbert\\checkpoint-10\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0013778619468212128, 'eval_runtime': 16.5922, 'eval_samples_per_second': 1.205, 'eval_steps_per_second': 0.181, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in finetune_sentiments_analysis_distilbert\\checkpoint-10\\pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from finetune_sentiments_analysis_distilbert\\checkpoint-10 (score: 0.0013778619468212128).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 118.7598, 'train_samples_per_second': 0.674, 'train_steps_per_second': 0.084, 'train_loss': 0.033715859055519104, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=0.033715859055519104, metrics={'train_runtime': 118.7598, 'train_samples_per_second': 0.674, 'train_steps_per_second': 0.084, 'train_loss': 0.033715859055519104, 'epoch': 1.0})"
            ]
          },
          "execution_count": 221,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in finetune_sentiments_analysis_distilbert\\config.json\n",
            "Model weights saved in finetune_sentiments_analysis_distilbert\\pytorch_model.bin\n",
            "tokenizer config file saved in finetune_sentiments_analysis_distilbert\\tokenizer_config.json\n",
            "Special tokens file saved in finetune_sentiments_analysis_distilbert\\special_tokens_map.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved successfully to local directory: finetune_sentiments_analysis_distilbert\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(\"Model saved successfully to local directory:\", output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 5: Inference Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.special import softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.txt\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading configuration file finetune_sentiments_analysis_distilbert\\config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"finetune_sentiments_analysis_distilbert\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"finetuning_task\": \"sst-2\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"NEGATIVE\",\n",
            "    \"1\": \"POSITIVE\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"NEGATIVE\": 0,\n",
            "    \"POSITIVE\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file finetune_sentiments_analysis_distilbert\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at finetune_sentiments_analysis_distilbert.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = load_model(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process the input text and return sentiment prediction\n",
        "def sentiment_analysis(text):\n",
        "    # text = preprocess(text)\n",
        "    encoded_input = tokenizer(text, return_tensors=\"pt\")  # for PyTorch-based models\n",
        "    output = model(**encoded_input)\n",
        "    scores_ = output[0][0].detach().numpy()\n",
        "    scores_ = softmax(scores_)\n",
        "\n",
        "    # Format output dictionary of scores\n",
        "    labels = [\"Negative\", \"Positive\"]\n",
        "    scores = {l: float(s) for (l, s) in zip(labels, scores_)}\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Negative': 0.0001110605662688613, 'Positive': 0.9998888969421387}"
            ]
          },
          "execution_count": 232,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_text = 'Scoot is awesome'\n",
        "\n",
        "sentiment_analysis(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Negative': 0.9997707009315491, 'Positive': 0.00022935708693694323}"
            ]
          },
          "execution_count": 234,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_text = 'Scoot is horrible'\n",
        "\n",
        "sentiment_analysis(sample_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
