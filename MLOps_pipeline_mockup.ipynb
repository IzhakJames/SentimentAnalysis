{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main resources:\n",
        "- https://brighteshun.medium.com/sentiment-analysis-part-1-finetuning-and-hosting-a-text-classification-model-on-huggingface-9d6da6fd856b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTaCz1i7WojQ"
      },
      "source": [
        "# 1. Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e_puNMeUsx2p"
      },
      "outputs": [],
      "source": [
        "# !pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "#finetuning\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline, AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer, DistilBertTokenizerFast, DefaultDataCollator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXM_PsOvWrrs"
      },
      "source": [
        "# 2. Run sentiment analysis predictions by using Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGGZsv-5sBxB"
      },
      "source": [
        "Data: https://www.airlinequality.com/airline-reviews/scoot/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iR3mTppxsGj7"
      },
      "outputs": [],
      "source": [
        "sample_data = 'I travelled with my sister, my elderly parent and my toddler son taking Scoot from Haikou to Singapore. The journey was a pleasant one despite the episode of my child became very unwell, alternating between drowsy and cranky in the mid of the journey. We sought the crew for help. They are very professional, helpful and friendly. They checked us out first, discussed with their captain and then prep us on the travel duration and preparation on ground. Besides, they offered ice pack/wet kitchen towels along the way. Nearing to the destination, they even moved me and my son to the front row. The flight arrived 20 minutes earlier. After that, they connected me and my son with the ground crew Firliza who accompanied us to airport clinic for medical treatment.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVAp8Urhs3Z7",
        "outputId": "710fe144-e463-4e73-c7b6-e345a30308be"
      },
      "outputs": [],
      "source": [
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWAcxn3HsLBh",
        "outputId": "55a41c51-9c22-4333-ac46-55884750ad7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9988247752189636}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = [sample_data]\n",
        "analysis = sentiment_pipeline(data)\n",
        "\n",
        "analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Fine tuning on custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_non_utf8(input_file, output_file, replace_char=''):\n",
        "  with open(input_file, 'r') as f:\n",
        "    content = f.read()\n",
        "  cleaned_content = ''.join(char if char.isascii() else replace_char for char in content)\n",
        "  with open(output_file, 'w') as f:\n",
        "    f.write(cleaned_content)\n",
        "\n",
        "\n",
        "def text_to_csv(input_file, output_file, delimiter=','):\n",
        "  \"\"\"\n",
        "  Converts a text file to a CSV file, using the specified delimiter.\n",
        "\n",
        "  Args:\n",
        "      input_file (str): Path to the input text file.\n",
        "      output_file (str): Path to the output CSV file.\n",
        "      delimiter (str, optional): Delimiter separating data in the text file. Defaults to ','.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(input_file, 'r') as input_file_handle, open(output_file, 'w', newline='') as output_csv:\n",
        "    reader = csv.reader(input_file_handle, delimiter=delimiter)\n",
        "    writer = csv.writer(output_csv)\n",
        "    # Assuming the first line contains headers (optional)\n",
        "    headers = next(reader)  # Read and store the header row (if present)\n",
        "    writer.writerow(headers)  # Write the header row to the CSV\n",
        "\n",
        "    for row in reader:\n",
        "      writer.writerow(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "data_path = 'base.csv'\n",
        "cleaned_txt_path = 'cleaned.txt'\n",
        "cleaned_csv_path = 'cleaned.csv'\n",
        "\n",
        "remove_non_utf8(data_path, cleaned_txt_path) \n",
        "text_to_csv(cleaned_txt_path, cleaned_csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Cleaned_Review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016</td>\n",
              "      <td>Gold Coast to Bangkok via Singapore with Scoot...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016</td>\n",
              "      <td>My Scoot flight from Melbourne to Singapore wa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016</td>\n",
              "      <td>Flew back from Amritsar to Singapore on 19th S...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016</td>\n",
              "      <td>$500 round trip from Tokyo to Taipei for a fam...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016</td>\n",
              "      <td>Overall excellent service from Scoot, however ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Year                                     Cleaned_Review  label\n",
              "0  2016  Gold Coast to Bangkok via Singapore with Scoot...      1\n",
              "1  2016  My Scoot flight from Melbourne to Singapore wa...      0\n",
              "2  2016  Flew back from Amritsar to Singapore on 19th S...      1\n",
              "3  2016  $500 round trip from Tokyo to Taipei for a fam...      1\n",
              "4  2016  Overall excellent service from Scoot, however ...      0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(cleaned_csv_path)\n",
        "\n",
        "# Rename 'old_name' to 'new_name'\n",
        "df = df.rename(columns={'is_negative_sentiment': 'label'})\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the train data => {train, eval}\n",
        "train, eval = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save splitted subsets\n",
        "\n",
        "df.to_csv(\"train.csv\", index=False)\n",
        "eval.to_csv(\"eval.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56bae6ae7dd54cabae87aaf673586442",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d02e0c2cb5764ef6aa850723195c6c07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7699a3ebf0bd4598b98206f28575ad58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb66055835404a1c91e54ecaa80ed708",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating eval split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset('csv',\n",
        "                        data_files =\n",
        "                        {'train': 'train.csv',\n",
        "                        'eval': 'eval.csv'\n",
        "                        }\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create a tokenizer instance\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "054b102114284cbd9867bacf926832ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1599 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "561f2d6ea64e4e278110318c82b2a23c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5aa34d7a19d94bfaa62d298ccd96d860",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1599 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8948ac3b52834a6abc0a47ae08d54e28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#create a function to convert label\n",
        "def transform_labels(label):\n",
        "    return {'labels': label['label']}\n",
        "\n",
        "# let's tokenize the data for the model to be able to understand\n",
        "def tokenize_data(example):\n",
        "    return tokenizer(example['Cleaned_Review'], truncation=True, padding='max_length')\n",
        "\n",
        "# Transform labels and remove the useless columns\n",
        "remove_label = ['label']\n",
        "remove_text = ['Cleaned_Review']\n",
        "\n",
        "dataset = dataset.map(transform_labels, remove_columns=remove_label)\n",
        "dataset = dataset.map(tokenize_data, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Year', 'Cleaned_Review', 'labels', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1599\n",
              "    })\n",
              "    eval: Dataset({\n",
              "        features: ['Year', 'Cleaned_Review', 'labels', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 320\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading a pretrain model while specifying the number of labels in our dataset for fine-tuning\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# freeze all layers except the final classifier\n",
        "for param in model.distilbert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.pre_classifier.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = \"finetune_sentiments_analysis_2\"\n",
        "#set the training arguments\n",
        "training_args = TrainingArguments(\n",
        "                            num_train_epochs=1,\n",
        "                            evaluation_strategy='epoch',\n",
        "                            save_strategy='epoch',\n",
        "                            learning_rate=2e-5,\n",
        "                            load_best_model_at_end=True,\n",
        "                            output_dir=output_dir\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# shuffle the datasets\n",
        "\n",
        "train_dataset = dataset['train'].shuffle(seed=10) \n",
        "eval_dataset = dataset['eval'].shuffle(seed=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: Year, Cleaned_Review. If Year, Cleaned_Review are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "C:\\Users\\jared\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1599\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 200\n",
            "  Number of trainable parameters = 1538\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f65665f06a2a476699861ec8285bb499",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: Year, Cleaned_Review. If Year, Cleaned_Review are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 320\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfc4a42b0c3940bdb853d21621c8896e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to finetune_sentiments_analysis_2\\checkpoint-200\n",
            "Configuration saved in finetune_sentiments_analysis_2\\checkpoint-200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.3411736488342285, 'eval_runtime': 230.8135, 'eval_samples_per_second': 1.386, 'eval_steps_per_second': 0.173, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in finetune_sentiments_analysis_2\\checkpoint-200\\pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from finetune_sentiments_analysis_2\\checkpoint-200 (score: 4.3411736488342285).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 2014.3307, 'train_samples_per_second': 0.794, 'train_steps_per_second': 0.099, 'train_loss': 4.2719595336914065, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=4.2719595336914065, metrics={'train_runtime': 2014.3307, 'train_samples_per_second': 0.794, 'train_steps_per_second': 0.099, 'train_loss': 4.2719595336914065, 'epoch': 1.0})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in finetune_sentiments_analysis_2\\config.json\n",
            "Model weights saved in finetune_sentiments_analysis_2\\pytorch_model.bin\n",
            "tokenizer config file saved in finetune_sentiments_analysis_2\\tokenizer_config.json\n",
            "Special tokens file saved in finetune_sentiments_analysis_2\\special_tokens_map.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved successfully to local directory: finetune_sentiments_analysis_2\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(\"Model saved successfully to local directory:\", output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 5: Inference Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.special import softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.txt\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading configuration file finetune_sentiments_analysis_2\\config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"finetune_sentiments_analysis_2\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"finetuning_task\": \"sst-2\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"NEGATIVE\",\n",
            "    \"1\": \"POSITIVE\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"NEGATIVE\": 0,\n",
            "    \"POSITIVE\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file finetune_sentiments_analysis_2\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at finetune_sentiments_analysis_2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = load_model(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process the input text and return sentiment prediction\n",
        "def sentiment_analysis(text):\n",
        "    # text = preprocess(text)\n",
        "    encoded_input = tokenizer(text, return_tensors=\"pt\")  # for PyTorch-based models\n",
        "    output = model(**encoded_input)\n",
        "    scores_ = output[0][0].detach().numpy()\n",
        "    scores_ = softmax(scores_)\n",
        "\n",
        "    # Format output dictionary of scores\n",
        "    labels = [\"Negative\", \"Positive\"]\n",
        "    scores = {l: float(s) for (l, s) in zip(labels, scores_)}\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Negative': 0.00037257905933074653, 'Positive': 0.9996273517608643}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_text = 'Scoot is awesome'\n",
        "\n",
        "sentiment_analysis(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Negative': 0.9993104934692383, 'Positive': 0.0006894702091813087}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_text = 'Scoot is horrible'\n",
        "\n",
        "sentiment_analysis(sample_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
